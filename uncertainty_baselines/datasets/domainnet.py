# coding=utf-8
# Copyright 2021 The Uncertainty Baselines Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""DomainNet Dataset."""

import os
from typing import Optional

from robustness_metrics.common import types
import tensorflow.compat.v2 as tf
import tensorflow_datasets as tfds
from uncertainty_baselines.datasets import base

_DESCRIPTION = """\
Cifar100Corrupted is a dataset generated by adding 17 corruptions to the test
images in the Cifar100 dataset.
"""

_CITATION = """\
"""

_CIFAR_CLASSES = 100
_NUM_EXAMPLES = 50000
_DOMAINS = ["sketch","real", "quickdraw", "painting", "infograph", "clipart"]

class DomainNetConfig(tfds.core.BuilderConfig):
  """BuilderConfig for Cifar100Corrupted."""

  def __init__(self, *, domain, **kwargs):
    """Constructor.

    Args:
      domain: string, must be one of the items in _DOMAINS.
      **kwargs: keyword arguments forwarded to super.
    """
    super().__init__(**kwargs)
    self.domain = domain


def _make_builder_configs():
  """Construct a list of BuilderConfigs.

  """
  config_list = []
  for domain in _DOMAINS:
      config_list.append(
          DomainNetConfig(
              name=domain,
              description='Domain: ' + domain,
              domain=domain,
          ))
  return config_list


class _DomainNetDatasetBuilder(tfds.core.DatasetBuilder):
  """DomainNet dataset."""
  VERSION = tfds.core.Version('1.0.0')
  RELEASE_NOTES = {
      '1.0.0': 'Initial release.',
  }
  BUILDER_CONFIGS = _make_builder_configs()

  def __init__(self, data_dir, **kwargs):
    super().__init__(
        data_dir=data_dir, **kwargs)
    # We have to override self._data_dir to prevent the parent class from
    # appending the class name and version.
    self._data_dir = data_dir

  def _info(self):
    """Returns basic information of dataset.
    Returns:
      tfds.core.DatasetInfo.
    """
    info = tfds.core.DatasetInfo(
        builder=self,
        description=_DESCRIPTION,
        features=tfds.features.FeaturesDict({
            'image': tfds.features.Tensor(shape=(224,224,3),
                                          dtype=tf.int64),
            'label': tfds.features.ClassLabel(num_classes=345),
        }),
        supervised_keys=('image', 'label'),
        homepage='https://github.com/hendrycks/robustness',
        citation=_CITATION)

    split_infos = [
        tfds.core.SplitInfo(
            name=tfds.Split.TEST,
            shard_lengths=[_NUM_EXAMPLES],
            num_bytes=0,
        ),
    ]
    split_dict = tfds.core.SplitDict(split_infos, dataset_name=self.name)
    info.set_splits(split_dict)
    return info



  def _download_and_prepare(self, dl_manager, download_config=None):
    """Downloads and prepares dataset for reading."""
    raise NotImplementedError(
        'Must provide a data_dir with the files already downloaded to.')

  def _as_dataset(
      self,
      split: tfds.Split,
      decoders=None,
      read_config=None,
      shuffle_files=False) -> tf.data.Dataset:
    """Constructs a `tf.data.Dataset`."""
    del decoders
    del read_config
    if split == tfds.Split.TRAIN:
     filename = (f'domainnet_{self._builder_config.domain}_train_*-of-*.tfrecords')
     filepath = os.path.join(self._data_dir, filename)
     files = tf.io.matching_files(filepath)
     if shuffle_files:
         files = tf.random.shuffle(files)
     shards = tf.data.Dataset.from_tensor_slices(files)
     dataset = shards.interleave(tf.data.TFRecordDataset)
     return dataset
    elif split == tfds.Split.TEST:
      filename = (f'domainnet_{self._builder_config.domain}_test_*-of-*.tfrecords')
      filepath = os.path.join(self._data_dir, filename)
      files = tf.io.matching_files(filepath)
      shards = tf.data.Dataset.from_tensor_slices(files)
      dataset = shards.interleave(tf.data.TFRecordDataset)
      return dataset
    raise ValueError('Unsupported split given: {}.'.format(split))


class DomainNetDataset(base.BaseDataset):
  """DomainNet dataset builder class."""

  def __init__(
      self,
      domain: str,
      split: str,
      shuffle_buffer_size = 1000,
      num_parallel_parser_calls: int = 64,
      drop_remainder: bool = True,
      data_dir: Optional[str] = None,
      download_data: bool = False):
    """Create a CIFAR100-C tf.data.Dataset builder.

    Args:
      domain: DomainNet domain
      split: a dataset split, either a custom tfds.Split or one of the
        tfds.Split enums [TRAIN, VALIDAITON, TEST] or their lowercase string
        names.
      num_parallel_parser_calls: the number of parallel threads to use while
        preprocessing in tf.data.Dataset.map().
      drop_remainder: whether or not to drop the last batch of data if the
        number of points is not exactly equal to the batch size. This option
        needs to be True for running on TPUs.
      data_dir: path to a directory containing the CIFAR dataset, with
        filenames '{corruption_name}_{corruption_severity}.tfrecords'.
      normalize: whether or not to normalize each image by the CIFAR dataset
        mean and stddev.
      download_data: Whether or not to download data before loading. Currently
        unsupported.
    """
    dataset_builder = _DomainNetDatasetBuilder(
        data_dir, config=f'{domain}')
    super().__init__(
        name=f'{dataset_builder.name}/{dataset_builder.builder_config.name}',
        dataset_builder=dataset_builder,
        split=split,
        num_parallel_parser_calls=num_parallel_parser_calls,
        drop_remainder=drop_remainder,
        shuffle_buffer_size=shuffle_buffer_size,
        download_data=download_data)

  def _create_process_example_fn(self) -> base.PreProcessFn:

    def _example_parser(example: types.Features) -> types.Features:
      """A pre-process function to return images in [0, 1]."""
      features = tf.io.parse_single_example(
          example['features'],
          features={
              'image': tf.io.FixedLenFeature([], tf.string),
              'label': tf.io.FixedLenFeature([], tf.int64),
          })
      dtype = tf.float32
      image = tf.io.decode_jpeg(features['image'])
      image = tf.image.convert_image_dtype(image, dtype)
      width = tf.cast(tf.shape(image),tf.float32)[1]
      height = tf.cast(tf.shape(image),tf.float32)[0]
      min_side = tf.minimum(width,height)
      ratio = tf.cast(min_side, tf.float32) / 256.

      resized_width = tf.cast((width /ratio), tf.int32)
      resized_height = tf.cast((height /ratio),tf.int32)
      print(width)
      print(height)
      print(ratio)
      print(resized_width)
      print(resized_height)
      image = tf.image.resize(image, [resized_height, resized_width])
      if self.split == tfds.Split.TRAIN:
          image = tf.image.random_crop(image, [224,224,3])
          image = tf.image.random_flip_left_right(image)
      else:
          offset_width = (resized_width - 224) // 2
          offset_height = (resized_height - 224) // 2
          image = tf.image.crop_to_bounding_box(image, offset_height, offset_width, 224,224)

      label = tf.cast(features['label'], dtype)
      return {
          'features': image,
          'labels': tf.cast(label, tf.int32),
      }

    return _example_parser
